{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import path, getcwd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_LEVEL_DIR = getcwd()\n",
    "TOP_LEVEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_PORT = 8691\n",
    "API_PORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PARAMS = {\n",
    "    # LLaVA 1.5-13B\n",
    "\n",
    "    # 8 bits\n",
    "    \"model_name\": \"TheBloke/llava-v1.5-13B-GPTQ:gptq-8bit-32g-actorder_True\",\n",
    "    \"model\": \"TheBloke_llava-v1.5-13B-GPTQ_gptq-8bit-32g-actorder_True\",\n",
    "\n",
    "    # 4 bits\n",
    "    #\"model_name\": \"TheBloke/llava-v1.5-13B-GPTQ:gptq-4bit-32g-actorder_True\",\n",
    "    #\"model\": \"TheBloke_llava-v1.5-13B-GPTQ_gptq-4bit-32g-actorder_True\",\n",
    "\n",
    "    # 4 bits - 128g - actorder_True\n",
    "    #\"model_name\": \"TheBloke/llava-v1.5-13B-GPTQ\",\n",
    "    #\"model\": \"TheBloke_llava-v1.5-13B-GPTQ\",\n",
    "\n",
    "\n",
    "    \"pipeline\": \"llava-v1.5-13b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER_CMD = [\n",
    "    \"bash\",\n",
    "    \"start_linux.sh\",\n",
    "    \"--model\", MODEL_PARAMS['model'],\n",
    "    \"--multimodal-pipeline\", MODEL_PARAMS['pipeline'],\n",
    "    \"--disable_exllama\",\n",
    "    \"--loader autogptq\", \"--no_inject_fused_attention\", # Fused attention causes an error\n",
    "    \"--api\", \"--api-port\", f\"{API_PORT}\",\n",
    "    \"--no-cache\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_NAME = \"run_seed_\" + MODEL_PARAMS['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "noconvert"
    ]
   },
   "outputs": [],
   "source": [
    "SCITAS_PARAMS = f\"\"\"#!/bin/bash -l\n",
    "\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --time 36:00:00\n",
    "#SBATCH --cpus-per-task=10\n",
    "#SBATCH --partition=gpu\n",
    "#SBATCH --qos=gpu\n",
    "#SBATCH --gres=gpu:2\n",
    "#SBATCH --mem 64G\n",
    "\n",
    "cd ~/tgw\n",
    "{' '.join(SERVER_CMD)} &\n",
    "cd ~/\n",
    "ipython {SCRIPT_NAME}.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "noconvert"
    ]
   },
   "outputs": [],
   "source": [
    "# Generate SCITAS job script\n",
    "with open(f'{SCRIPT_NAME}.run', 'w+') as job_file:\n",
    "    job_file.write(SCITAS_PARAMS)\n",
    "\n",
    "# Generate actual python script\n",
    "!jupyter nbconvert --to script run_seed.ipynb --output {SCRIPT_NAME} \\\n",
    "    -TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags noconvert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {TOP_LEVEL_DIR}/qvlm\n",
    "\n",
    "questions_path = 'datasets/SEED/SEED-Bench.json'\n",
    "questions_json = json.load(open(questions_path))\n",
    "questions_df = pd.DataFrame(questions_json['questions'])\n",
    "questions_df = questions_df.loc[questions_df['question_type_id'] < 10] # Only image questions (dimensions 1-9)\n",
    "questions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Launching the model server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {TOP_LEVEL_DIR}/tgw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "%cd {TOP_LEVEL_DIR}/tgw\n",
    "\n",
    "def get_model_server_process(params: dict):\n",
    "  if (not path.exists(params['model'])):\n",
    "    !python download-model.py {params['model_name']}\n",
    "  return lambda: subprocess.run(SERVER_CMD, check=True, shell=True, close_fds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {TOP_LEVEL_DIR}/qvlm\n",
    "\n",
    "import socket,time\n",
    "from eval.connectors import Connector\n",
    "\n",
    "def wait_for_port(connector: Connector, delay: int = 3, max_retries: int = 1000):\n",
    "  sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "  conn_info = (connector.url, connector.port)\n",
    "  result = sock.connect_ex(conn_info)\n",
    "  counter = max_retries\n",
    "  while (counter >= 0 and result != 0):\n",
    "    print(f\"Port is not open, retrying in {delay}s...\\t({max_retries - counter}/{max_retries})\")\n",
    "    time.sleep(delay)\n",
    "    result = sock.connect_ex(conn_info)\n",
    "    counter = counter - 1\n",
    "  \n",
    "  if (result == 0):\n",
    "    print(\"Port is open!\")\n",
    "    sock.close()\n",
    "  else:\n",
    "    print(f\"Port was not open after n={max_retries} max retries\")\n",
    "    sock.close()\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd {TOP_LEVEL_DIR}/tgw\n",
    "# Do not uncomment, this doesn't work yet\n",
    "#threading.Thread(target=get_model_server_process(PARAMS), daemon=True).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Computing the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {TOP_LEVEL_DIR}/qvlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from eval.connectors.llamafile import LlamafileConnector\n",
    "from eval.connectors.textgenerationwebui import TextGenerationWebUIConnector\n",
    "\n",
    "connector = TextGenerationWebUIConnector('127.0.0.1', API_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval.evaluation.SEED import SEED1Evaluator\n",
    "\n",
    "evaluator = SEED1Evaluator(questions_df, img_dir='datasets/SEED/SEED-Bench-image')\n",
    "evaluator.connect(connector)\n",
    "\n",
    "#wait_for_port(connector, delay=3, max_retries = 200)\n",
    "#time.sleep(30) # The server can take some time to keep booting after the port has been opened...\n",
    "#evaluator.get_responses(f'datasets/SEED/{MODEL_PARAMS[\"model\"]}_responses.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.eval(f'datasets/SEED/{MODEL_PARAMS[\"model\"]}_responses.jsonl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm_q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
